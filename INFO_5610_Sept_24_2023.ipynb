{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM517BaunX0jeqnZ5U5MnlR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadler6/info5610_studentlife/blob/master/INFO_5610_Sept_24_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We are going to imitate a feature in the [HealthRhythms API](https://rest.docs.healthrhythms.com/#/operations/get_summary_v2_data_summary_get) using location data from the public [StudentLife](https://studentlife.cs.dartmouth.edu/dataset.html) dataset. Remember, we read [the StudentLife paper in class](https://dl.acm.org/doi/10.1145/2632048.2632054). We will see if the feature we create is associated with a stress EMA  response.\n",
        "\n",
        "Let's start by importing some helpful data manipulation libraries."
      ],
      "metadata": {
        "id": "oXGX6z6mCRI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For data manipulation\n",
        "import pandas as pd # Pandas\n",
        "import numpy as np # Numpy"
      ],
      "metadata": {
        "id": "XMD7V-7iDQMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading data\n",
        "\n",
        "We can upload raw StudentLife data using the code below. I put the data we will use in a public GitHub repository, [linked](https://github.com/dadler6/info5610_studentlife)."
      ],
      "metadata": {
        "id": "AtcGy9Q2CxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to github\n",
        "github_url = \"https://raw.githubusercontent.com/dadler6/info5610_studentlife/master/\"\n",
        "\n",
        "# Participant IDs\n",
        "participant_ids = [\n",
        "    'u00', 'u10', 'u20', 'u33', 'u45', 'u56',\n",
        "    'u01', 'u12', 'u22', 'u34', 'u46', 'u57',\n",
        "    'u02', 'u13', 'u23', 'u35', 'u47', 'u58',\n",
        "    'u03', 'u14', 'u24', 'u36', 'u49', 'u59',\n",
        "    'u04', 'u15', 'u25', 'u39', 'u50',\n",
        "    'u05', 'u16', 'u27', 'u41', 'u51',\n",
        "    'u07', 'u17', 'u30', 'u42', 'u52',\n",
        "    'u08', 'u18', 'u31', 'u43', 'u53',\n",
        "    'u09', 'u19', 'u32', 'u44', 'u54']\n",
        "\n",
        "# Upoload data\n",
        "data = dict()\n",
        "for data_type in ['gps', 'Stress']:\n",
        "    # Create dict\n",
        "    data[data_type] = []\n",
        "    # Check data_type and append file type\n",
        "    for i in participant_ids:\n",
        "        # GPS is csv\n",
        "        if data_type == 'gps':\n",
        "            df = pd.read_csv(\n",
        "                github_url + data_type + '/' + \\\n",
        "                data_type + '_' + i + '.csv'\n",
        "            )\n",
        "        # Stress is JSON\n",
        "        else:\n",
        "            df = pd.read_json(\n",
        "                github_url + data_type + '/' + \\\n",
        "                data_type + '_' + i + '.json'\n",
        "            )\n",
        "        # Add Participant ID\n",
        "        df['pid'] = i\n",
        "        # Save\n",
        "        data[data_type].append(df)"
      ],
      "metadata": {
        "id": "0snJq0neC3v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's concatenate the data, see how large of a dataset we have, and print the first 5 rows of data.\n",
        "\n",
        "In your own project, it may also be good to look at the number of unique participants, as well as how much data exists per participant.\n",
        "\n",
        "**What else may be good to understand when you initially upload data?**"
      ],
      "metadata": {
        "id": "fP2OBYpRPx9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPS\n",
        "data['gps'] = pd.concat(data['gps']).reset_index()\n",
        "print('Shape', data['gps'].shape)\n",
        "data['gps'].head()"
      ],
      "metadata": {
        "id": "NGPPvPeHO8wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stress\n",
        "data['Stress'] = pd.concat(data['Stress']).reset_index()\n",
        "print('Shape', data['Stress'].shape)\n",
        "data['Stress'].head()"
      ],
      "metadata": {
        "id": "bvjGIML3TCZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "It looks like some of the columns are not aligned wtih the data contained within the column! Let's clean them up a bit."
      ],
      "metadata": {
        "id": "Crenp6yATIu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the GPS data, it appears the time was an index, initially,\n",
        "# which shifted when I ran the reset_index() function.\n",
        "# Lets retrieve the columns we are interested in and name them something more ideal...\n",
        "\n",
        "data['gps'] = data['gps'][['pid', 'index', 'accuracy', 'latitude']]\n",
        "data['gps'].columns = ['pid', 'datetime', 'latitude', 'longitude']"
      ],
      "metadata": {
        "id": "9544x_ZJTGu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In the Stress data, as per the documentation, we only want to keep the\n",
        "# \"resp_time\", and \"level\" columns\n",
        "data['Stress'] = data['Stress'][['pid', 'resp_time', 'level', 'location']]\n",
        "\n",
        "# Let's also rename the columns such that the names align with\n",
        "# GPS data, and we can give the \"level\" column a more descriptive name\n",
        "data['Stress'].columns = ['pid', 'datetime', 'ema_stress', 'location']"
      ],
      "metadata": {
        "id": "kJSLc7xrWn4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to extract relevant datetime information. The documentation says that the datetime is a UNIX time. We can localize the time by using the location data in each dataframe.\n",
        "\n",
        "We'll use the `timezonefinder` library to do this. Let's install it in our kernel and then import it.\n"
      ],
      "metadata": {
        "id": "uL_ovM-dXqaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For datetime\n",
        "!pip install timezonefinder\n",
        "import timezonefinder"
      ],
      "metadata": {
        "id": "ayZdI9HPZM5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a timezonefinder object."
      ],
      "metadata": {
        "id": "6Wfb6OaDZf3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tz_finder = timezonefinder.TimezoneFinder()"
      ],
      "metadata": {
        "id": "j-y_H4sqXjKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And get the timzeones of the GPS data to localize it."
      ],
      "metadata": {
        "id": "ZlesqJJ4ZmRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['gps']['timezone'] = [tz_finder.timezone_at(\n",
        "    lng=data['gps'].loc[ind, 'longitude'],\n",
        "    lat=data['gps'].loc[ind, 'latitude']\n",
        ") for ind in data['gps'].index]"
      ],
      "metadata": {
        "id": "coT-MLUGZktg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Stress datetime column is a little trickier to manipulate because the location is listed as a string in a single column, separated by a comma. We'll have to separate the string first and extract the latitude/longitude coordinates.\n",
        "\n",
        "In addition, there are some NA values in this column. Let's drop those for the purposes of this particular lecture, but you may want to think about missing data filling for your project.\n",
        "\n",
        "**What are the benefits/downsides of filling missing Stress EMA data? There may be GPS data that aligns with the missing data...**"
      ],
      "metadata": {
        "id": "axacLflkaphI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop NA rows\n",
        "data['Stress'] = data['Stress'].dropna()\n",
        "\n",
        "# There are also some rows where location is not known\n",
        "# filter these out\n",
        "data['Stress'] = data['Stress'].loc[\n",
        "    data['Stress']['location'] != 'Unknown', :\n",
        "]\n",
        "\n",
        "# Reset the index and create a new column\n",
        "data['Stress'] = data['Stress'].reset_index(drop=True)\n",
        "data['Stress']['timezone'] = None\n",
        "\n",
        "\n",
        "# Iterate through each remaining index\n",
        "for ind in data['Stress'].index:\n",
        "    # Split value by comma\n",
        "    lat, lon = data['Stress'].loc[ind, 'location'].split(',')\n",
        "    # Get timezone\n",
        "    data['Stress'].loc[ind, 'timezone'] = \\\n",
        "        tz_finder.timezone_at(\n",
        "            lng=float(lon), lat=float(lat)\n",
        "        )"
      ],
      "metadata": {
        "id": "S7AVgQqYapDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One other funky thing about the stress data. This is the current meaning of the values in this column:\n",
        "\n",
        "1. A little stressed\n",
        "2. Definitely stressed\n",
        "3. Stressed out\n",
        "4. Feeling good\n",
        "5. Feeling great\n",
        "\n",
        "We should map these to values that are a bit more...ordered. We can perform the following mapping to simplify this problem a bit, between feeling good/great versus definitely stressed/stress out.\n",
        "\n",
        "The \"a little stressed\" response is a bit ambiguous. Only analyzing extremes will make our lives easier for today."
      ],
      "metadata": {
        "id": "bqe2_VvPqcEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Stress']['ema_stress'] = data['Stress']['ema_stress'].map({\n",
        "    5: 0,\n",
        "    4: 0,\n",
        "    1: None,\n",
        "    2: 1,\n",
        "    3: 1\n",
        "})\n",
        "\n",
        "data['Stress'] = data['Stress'].dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "tt4UWlOJqoVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datetime manipulation and localization\n",
        "\n",
        "Now that the columns are cleaned, let's localize the time. First, we'll check the dtypes of each dataframe, to see if Pandas has inferred a datetime object previously."
      ],
      "metadata": {
        "id": "xeUFvcjnclaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['gps'].dtypes, data['Stress'].dtypes"
      ],
      "metadata": {
        "id": "_uNnTb1Db47-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like Pandas has inferred a datetime object for the Stress EMA data, but not for the GPS data. We'll have to first convert the gps data to a datetime before localizing the timezone."
      ],
      "metadata": {
        "id": "ZiE1pfg_cw39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through each df\n",
        "for data_type in data.keys():\n",
        "    # If GPS, create datetime\n",
        "    if data_type == 'gps':\n",
        "        # In UNIX seconds timestamp\n",
        "        data['gps']['datetime'] = pd.to_datetime(data['gps']['datetime'], unit='s')\n",
        "    # Now localize\n",
        "    data[data_type]['loc_datetime'] = None\n",
        "    for ind in data[data_type].index:\n",
        "        data[data_type].loc[ind, 'loc_datetime'] = \\\n",
        "            data[data_type].loc[ind, 'datetime'].tz_localize(\"UTC\").tz_convert(\n",
        "                data[data_type].loc[ind, 'timezone'])"
      ],
      "metadata": {
        "id": "XeuZtUn2cwWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Now that we have two dataframes, let's create a feature! We will focus on the feature `activity_percent`, which measures the percentage of time active during the day.\n",
        "\n",
        "We'll use the feature creation methods specified in [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4526997/)."
      ],
      "metadata": {
        "id": "DBx5qpXxeSxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity Percentage\n",
        "\n",
        "We'll calculate \"activity_percent\" similar to the \"Transition Time\" listed in the paper. The paper states.\n",
        "\n",
        "\"Transition time represented the percentage of time during which a participant was in a non-stationary state (see Data Preprocessing). This was calculated by dividing the number of GPS location samples in transition states by the total number of samples.\"\n",
        "\n",
        "To find non-stationary states, the paper does the following:\n",
        "\n",
        "\"To do so, we estimated the movement speed at each location data sample by calculating its time derivative and then used a threshold speed that defined the boundary between these two states. In this study, we set this threshold to 1 km/h.\"\n",
        "\n",
        "Let's define a function together called activity_percent to identify the percentage of \"active\" data points (moving >1 km/h) using the latitude, longitude columns. The function will output the percentage of active data points.\n",
        "\n",
        "To do this, we'll also need to calculate the distance between two latitude, longitude points, which we can approximate using the [Haversine Formula](https://en.wikipedia.org/wiki/Haversine_formula).\n",
        "\n",
        "**How could this feature be improved?**\n",
        "\n",
        "**When you build a feature, how can you check that the function performs correctly?**"
      ],
      "metadata": {
        "id": "TUlpm36Rhbwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distance(origin, destination):\n",
        "    \"\"\"\n",
        "    Compute distance between two points\n",
        "\n",
        "    :param origin: (<float>, <float>) lat, long location origin\n",
        "    :param destination: (<float>, <float>) lat, long location destination\n",
        "\n",
        "    :return: <float> the distance in km between points\n",
        "    \"\"\"\n",
        "    lat1, lon1 = origin\n",
        "    lat2, lon2 = destination\n",
        "    radius = 6371 # km\n",
        "\n",
        "    dlat = np.radians(lat2-lat1)\n",
        "    dlon = np.radians(lon2-lon1)\n",
        "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat1)) \\\n",
        "        * np.cos(np.radians(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    d = radius * c\n",
        "\n",
        "    # Return distances in kilometers\n",
        "    return d\n",
        "\n",
        "def activity_percent(df):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of data points \"in transition\".\n",
        "    \"In transition\" defined as moving >1 km/hour\n",
        "\n",
        "    :param df: pd.DataFrame, GPS data for a specific user\n",
        "                             with latitude, longitude columns,\n",
        "                             and loc_datetime column represented\n",
        "                             the localized datetime\n",
        "    :return: float, the percentage of data points in transition\n",
        "    \"\"\"\n",
        "    # Sort dataframe\n",
        "    df = df.sort_values(by='loc_datetime').reset_index(drop=True)\n",
        "\n",
        "    # Make distance, timediff, moving cols\n",
        "    df['dist'] = None\n",
        "    df['timediff'] = None\n",
        "\n",
        "    # Iterate through rows after\n",
        "    # first index\n",
        "    for ind in df.index[1:]:\n",
        "        # Get information\n",
        "        loc1 = df.loc[ind - 1, ['latitude', 'longitude']]\n",
        "        loc2 = df.loc[ind, ['latitude', 'longitude']]\n",
        "        t1 = df.loc[ind - 1, 'loc_datetime']\n",
        "        t2 = df.loc[ind, 'loc_datetime']\n",
        "\n",
        "        # Get distance\n",
        "        df.loc[ind, 'dist'] = distance(loc1, loc2)\n",
        "        # Get timediff in hours\n",
        "        df.loc[ind, 'timediff'] = (t2 - t1).seconds / 3600\n",
        "\n",
        "    # Drop na\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Get speed\n",
        "    df['speed'] = df['dist'] / df['timediff']\n",
        "\n",
        "    # > 1 km/hour = moving\n",
        "    df['moving'] = 0\n",
        "    df.loc[df['speed'] > 1, 'moving'] = 1\n",
        "\n",
        "    # Return active_ime\n",
        "    return df['moving'].mean()"
      ],
      "metadata": {
        "id": "zVMilchXha9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create some features, we need to make sure they align with the outcome data, in this case the stress data. We'll also need to choose a retrospective time window in which to create features within.\n",
        "\n",
        "The Stress EMA states \"Right now, I am...\", so it is measuring momentary stress. Therefore, we should probably use location data pretty close to the timeframe to assess the answer to this question.\n",
        "\n",
        "For convenience, maybe we'll use 8 hours of data prior to each stress EMA answered to respond to this question. We'll need the datetime library to manipulate the dates. Let's import it.\n",
        "\n",
        "**What may be better ways to define a retrospective time window??**"
      ],
      "metadata": {
        "id": "pltaK5weg8us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "x8sunMvmgJJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's iterate through each stress EMA recorded, and estimate the active time in the 8 hours before each stress EMA was recorded."
      ],
      "metadata": {
        "id": "KbmdSwSmgNEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For holding cleaned data\n",
        "cleaned_dfs = []\n",
        "\n",
        "# Iterate through each pid\n",
        "for pid in data['Stress']['pid'].unique():\n",
        "    # Get data for a pid\n",
        "    stress_df = data['Stress'].loc[data['Stress']['pid'] == pid, :]\n",
        "    gps_df = data['gps'].loc[data['gps']['pid'] == pid, :]\n",
        "\n",
        "    # Reset index\n",
        "    stress_df = stress_df.reset_index(drop=True)\n",
        "    gps_df = gps_df.reset_index(drop=True)\n",
        "\n",
        "    # Create active time col\n",
        "    stress_df['activity_percent'] = None\n",
        "\n",
        "    # Go through each stress EMA answered\n",
        "    for ind in stress_df.index:\n",
        "        # Get datetime of the stress response\n",
        "        end_dt = stress_df.loc[ind, 'loc_datetime']\n",
        "        # Get 8 hours before this datetime\n",
        "        start_dt = end_dt - timedelta(hours=8)\n",
        "\n",
        "        # Get location data with this datetime\n",
        "        gps_df_temp = gps_df.loc[\n",
        "            (gps_df.loc_datetime >= start_dt) &\n",
        "            (gps_df.loc_datetime <= end_dt)\n",
        "        ]\n",
        "        # Check to see if data exists, if it does create features\n",
        "        if gps_df_temp.shape[0] > 0:\n",
        "            stress_df.loc[ind, 'activity_percent'] = activity_percent(df=gps_df_temp.copy())\n",
        "\n",
        "    # Save stress_df\n",
        "    cleaned_dfs.append(\n",
        "        stress_df[['pid', 'loc_datetime', 'ema_stress', 'activity_percent']].dropna()\n",
        "    )"
      ],
      "metadata": {
        "id": "q7L6O4kcd06F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's concatenate together the cleaned DataFrames, and see how much data we have left."
      ],
      "metadata": {
        "id": "3GXnpgqrnRzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate\n",
        "cleaned_df = pd.concat(cleaned_dfs).reset_index(drop=True)\n",
        "# Make sure column is a flot\n",
        "cleaned_df['activity_percent'] = cleaned_df['activity_percent'].astype(float)\n",
        "# Check results\n",
        "print('Shape', cleaned_df.shape)\n",
        "cleaned_df.head()"
      ],
      "metadata": {
        "id": "hu6-86hDgmKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "There are many different ways to analyze data, including visualizations, statistical models, and machine learning models."
      ],
      "metadata": {
        "id": "Zb2aZs10pzjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visuals\n",
        "\n",
        "Let's create some simple visuals to analyze our data. First, let's simply plot the average activity percentage per binary stress category.\n",
        "\n",
        "We can import the pyplot as seaborn libraries as our visualization libraries."
      ],
      "metadata": {
        "id": "Kfeq-_UPp9NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "xLlutfP-nz9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the distribution of `activity_percent` per stress response value. We'll also clean up the labels for readability."
      ],
      "metadata": {
        "id": "ObZDQtbxshAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_activity_percent(df):\n",
        "    \"\"\"\n",
        "    Plot activity percent boxplots\n",
        "\n",
        "    :param df: pd.DataFrame, the df to plot\n",
        "               with ema_stress and activity_percent\n",
        "               columns\n",
        "    \"\"\"\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(5, 3))\n",
        "\n",
        "    # Plot boxplot\n",
        "    sns.boxplot(\n",
        "        x=df['ema_stress'],\n",
        "        y=df['activity_percent'],\n",
        "    )\n",
        "\n",
        "    # Label axes\n",
        "    plt.xlabel('Stress EMA')\n",
        "    plt.ylabel('Active Time')\n",
        "\n",
        "    sns.despine()"
      ],
      "metadata": {
        "id": "Ichj10QwsgKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_activity_percent(cleaned_df)"
      ],
      "metadata": {
        "id": "7XEazft4txgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, it's kind of hard to see differences. Maybe we can look at data for specific individuals who have a lot of data."
      ],
      "metadata": {
        "id": "JN7T-ZB0tc5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_activity_percent(cleaned_df.loc[cleaned_df.pid == 'u59', :])"
      ],
      "metadata": {
        "id": "8KwRR6iAta6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_activity_percent(cleaned_df.loc[cleaned_df.pid == 'u16', :])"
      ],
      "metadata": {
        "id": "WWqs1fk40iMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics\n",
        "\n",
        "It looks like some differences exist at an individual level. Maybe we can quantify these differences using a regression library.\n",
        "\n",
        "We'll use the [statsmodels logit](https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html) function, which calculates logistic regression coefficients."
      ],
      "metadata": {
        "id": "_vdSbMQE0y4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "from statsmodels.discrete.discrete_model import Logit\n",
        "\n",
        "# Calculate coefficient\n",
        "cleaned_df['bias'] = 1\n",
        "logit = Logit(\n",
        "    endog=cleaned_df['ema_stress'],\n",
        "    exog=cleaned_df[['activity_percent', 'bias']]\n",
        ")\n",
        "# Run\n",
        "res = logit.fit()\n",
        "\n",
        "# Print summary\n",
        "res.summary()"
      ],
      "metadata": {
        "id": "GE10IbtA06ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there is a significant difference! Logistic regression coefficeints are a bit difficult to interpret.  \n",
        "\n",
        "We can transform these differences into [odds ratios](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/), which are a bit more interpretable. Odds ratios, in this case, would represent the expected activity percentage for stressed individuals divided by the expected activity percentage for non-stressed individuals."
      ],
      "metadata": {
        "id": "uGNcqSC62DZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oddsratio = np.exp(res.params).loc['activity_percent']\n",
        "conf_int = np.exp(res.conf_int()).loc['activity_percent', :]\n",
        "\n",
        "pd.Series(\n",
        "    [oddsratio, conf_int.loc[0], conf_int.loc[1]],\n",
        "    index=['OR', '2.75%', '97.5%'],\n",
        ")"
      ],
      "metadata": {
        "id": "sGHzWTJX1GeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the estimated activity percentage of someone experiencing stress is OR=0.08 95% CI (0.025 -- 0.28) times the activity percentage of someone not experiencing stress.\n",
        "\n",
        "**Based upon the visuals, is this the coefficient you would expect?**"
      ],
      "metadata": {
        "id": "bxBdkRr5335Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accounting for repeated measures\n",
        "\n",
        "One thing to account for in this setting is that we technically do not have independently, identically distributed variables (called IID), and thus logistic regression or similar linear models should not be used unless we somehow account for correlated measures.\n",
        "\n",
        "Generalized estimating equations (GEE) is a similar type of linear model that allows us to model correlations between subjects, and thus better estimate the population-wide coefficients. We can use the statsmodels [GEE implementation](https://www.statsmodels.org/stable/gee.html)."
      ],
      "metadata": {
        "id": "Km74WB0-4_MQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.genmod.generalized_estimating_equations import GEE\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# We will need to tell that model that \"pid\" is a grouping variable\n",
        "# and that the response variable is binary (Binomial)\n",
        "\n",
        "# Calculate coefficient\n",
        "gee = GEE(\n",
        "    endog=cleaned_df['ema_stress'],\n",
        "    exog=cleaned_df[['activity_percent', 'bias']],\n",
        "    groups=cleaned_df['pid'],\n",
        "    family=sm.families.Binomial(),\n",
        "    cov_struct=sm.cov_struct.Exchangeable() # The covariance model\n",
        ")\n",
        "\n",
        "# For more on covariance models, see:\n",
        "# https://online.stat.psu.edu/stat504/lesson/12/12.1\n",
        "\n",
        "# Run\n",
        "gee_res = gee.fit()\n",
        "\n",
        "# Print summary\n",
        "gee_res.summary()"
      ],
      "metadata": {
        "id": "qJwRWMAZ4XwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the GEE coefficeints are not significant!\n",
        "\n",
        "**Why is this??** Think: we want the regression coefficient to approximate the population effect independent of any specific individual in the population...\n",
        "\n",
        "Thinking about the below data may help...\n",
        "\n",
        "| pid | activity_percent | ema_stress |\n",
        "|-|-|-|\n",
        "|0|0.50|0|\n",
        "|0|0.50|0|\n",
        "|0|0.45|1|\n",
        "|1|0.10|1|\n",
        "|1|0.10|1|\n",
        "|1|0.15|0|"
      ],
      "metadata": {
        "id": "UfrGK_Bs7B96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn\n",
        "\n",
        "Use this time to come up with other ways to visualize or model the activity percentage that may provide a better fit. A few thoughts:\n",
        "\n",
        "* Based upon our visualizations, it looks like activity percentage changes with stress, but maybe not in the same direction per individual. How would you model this? What other analyses would you perform to focus more on individual-level differences?\n",
        "* Feel free to improve the feature, as well, or create other features from the [HealthRhythms API](https://rest.docs.healthrhythms.com/#/operations/get_summary_v2_data_summary_get)."
      ],
      "metadata": {
        "id": "NapgURM07KMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# More code here"
      ],
      "metadata": {
        "id": "dA0NEKjp-6yH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}